{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: einops in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: filelock in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\embeddapi\\.venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev\\embeddAPI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs =[\"\"\"\n",
    "The History and Impact of Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANNs) represent a fundamental shift in how we approach computation and artificial intelligence. Inspired by biological neural networks, these systems have evolved from simple perceptrons in the 1950s to today's sophisticated deep learning architectures.\n",
    "\n",
    "Early Development (1940s-1950s):\n",
    "The first artificial neuron was proposed by Warren McCulloch and Walter Pitts in 1943. Their mathematical model showed how neurons might work, demonstrating that simple neural networks could compute basic logical functions. In 1957, Frank Rosenblatt developed the perceptron, the first algorithm that could learn specific patterns through iterative training.\n",
    "\n",
    "The AI Winter (1970s):\n",
    "Despite early promise, neural network research faced significant setbacks in the 1970s. Marvin Minsky and Seymour Papert's 1969 book \"Perceptrons\" highlighted fundamental limitations of single-layer networks, particularly their inability to solve the XOR problem. This led to reduced funding and interest in neural network research, a period known as the \"AI Winter.\"\n",
    "\n",
    "Renaissance (1980s-1990s):\n",
    "The field experienced a revival with several breakthrough developments:\n",
    "1. The backpropagation algorithm became widely recognized as a solution for training multi-layer networks\n",
    "2. Improvements in computer processing power made larger networks feasible\n",
    "3. New architectures like Convolutional Neural Networks (CNNs) emerged\n",
    "4. Successful applications in pattern recognition and speech processing demonstrated practical value\n",
    "\n",
    "Modern Era (2000s-Present):\n",
    "The explosion of big data and computational power has led to remarkable achievements:\n",
    "- Deep learning models have surpassed human performance in various tasks\n",
    "- Applications range from computer vision to natural language processing\n",
    "- Transfer learning has enabled more efficient model training\n",
    "- Architectures like transformers have revolutionized language models\n",
    "\n",
    "Technical Foundations:\n",
    "\n",
    "Neural networks consist of interconnected layers of nodes, each performing weighted calculations:\n",
    "1. Input Layer: Receives raw data\n",
    "2. Hidden Layers: Process information through weighted connections\n",
    "3. Output Layer: Produces final results\n",
    "\n",
    "Key concepts include:\n",
    "- Activation functions (ReLU, sigmoid, tanh)\n",
    "- Weight initialization and adjustment\n",
    "- Loss functions and optimization algorithms\n",
    "- Regularization techniques\n",
    "\n",
    "Practical Applications:\n",
    "\n",
    "Modern neural networks have found applications across numerous fields:\n",
    "* Healthcare: Disease diagnosis, drug discovery, medical image analysis\n",
    "* Finance: Risk assessment, fraud detection, algorithmic trading\n",
    "* Transportation: Autonomous vehicles, traffic prediction, route optimization\n",
    "* Entertainment: Content recommendations, game AI, art generation\n",
    "\n",
    "Challenges and Future Directions:\n",
    "\n",
    "Despite their success, neural networks face several ongoing challenges:\n",
    "1. Interpretability and explainability of decisions\n",
    "2. Energy consumption and computational requirements\n",
    "3. Data privacy and ethical considerations\n",
    "4. Robustness against adversarial attacks\n",
    "\n",
    "Research continues in areas such as:\n",
    "- More efficient architectures\n",
    "- Unsupervised learning approaches\n",
    "- Neuromorphic computing\n",
    "- Integration with symbolic AI systems\n",
    "\n",
    "The field of neural networks continues to evolve rapidly, with new architectures and applications emerging regularly. As our understanding of both biological and artificial neural networks deepens, we can expect further innovations in this transformative technology.  \n",
    "    \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([740, 1024]), torch.Size([740, 2]), torch.Size([740]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def get_embedding(\n",
    "    docs, \n",
    "    task='retrieval.passage', \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", use_fast=True)\n",
    "    \n",
    "    tokens = tokenizer(\n",
    "        docs,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=False,\n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    task_id = model._adaptation_map[task]\n",
    "    adapter_mask = torch.full((len(docs),), task_id, dtype=torch.int32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens.input_ids, \n",
    "                       attention_mask=tokens.attention_mask,\n",
    "                       adapter_mask=adapter_mask,\n",
    "                       return_dict=True)\n",
    "\n",
    "    token_embeddings = outputs.last_hidden_state[0]\n",
    "    offsets_mapping = tokens.offset_mapping[0]\n",
    "    attention_mask = tokens.attention_mask[0]\n",
    "\n",
    "    return token_embeddings, offsets_mapping, attention_mask\n",
    "\n",
    "token_embeddings, offsets_mapping, attention_mask = get_embedding(docs)\n",
    "token_embeddings.shape, offsets_mapping.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.int64(0), np.int64(184)),\n",
       " (np.int64(185), np.int64(284)),\n",
       " (np.int64(285), np.int64(430)),\n",
       " (np.int64(431), np.int64(541)),\n",
       " (np.int64(542), 739)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimal_segmentation(values, min_chunk_size, max_chunk_size):\n",
    "    n = len(values)\n",
    "    similarity_matrix = np.dot(values, values.T)\n",
    "    mean_similarity = np.mean(similarity_matrix[np.triu_indices(similarity_matrix.shape[0], k=1)])\n",
    "    similarity_matrix = similarity_matrix - mean_similarity\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "\n",
    "    dp = np.zeros(n)\n",
    "    segmentation = np.zeros(n, dtype=int)\n",
    "\n",
    "    for i in range(n):\n",
    "        max_reward = float('-inf')\n",
    "        best_start = i\n",
    "\n",
    "        for size in range(min_chunk_size, min(max_chunk_size + 1, i + 2)):\n",
    "            if i - size + 1 >= 0:\n",
    "                reward = np.sum(similarity_matrix[i - size + 1:i + 1, i - size + 1:i + 1])\n",
    "                if i - size >= 0:\n",
    "                    reward += dp[i - size]\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    best_start = i - size + 1\n",
    "\n",
    "        dp[i] = max_reward\n",
    "        segmentation[i] = best_start\n",
    "\n",
    "    boundaries = []\n",
    "    i = n - 1\n",
    "    while i >= 0:\n",
    "        boundaries.append((segmentation[i], i))\n",
    "        i = segmentation[i] - 1\n",
    "\n",
    "    boundaries.reverse()\n",
    "    return boundaries\n",
    "\n",
    "\n",
    "min_chunk_size = 100\n",
    "max_chunk_size = 200\n",
    "\n",
    "embeddings = token_embeddings.numpy()\n",
    "\n",
    "boundaries = optimal_segmentation(embeddings, min_chunk_size, max_chunk_size)\n",
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(model_output.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"The History and Impact of Artificial Neural Networks\\n\\nArtificial Neural Networks (ANNs) represent a fundamental shift in how we approach computation and artificial intelligence. Inspired by biological neural networks, these systems have evolved from simple perceptrons in the 1950s to today's sophisticated deep learning architectures.\\n\\nEarly Development (1940s-1950s):\\nThe first artificial neuron was proposed by Warren McCulloch and Walter Pitts in 1943. Their mathematical model showed how neurons might work, demonstrating that simple neural networks could compute basic logical functions. In 1957, Frank Rosenblatt developed the perceptron, the first algorithm that could learn specific patterns through iterative training.\\n\\nThe AI Winter (1970s):\\nDespite early promise, neural network research faced significant setbacks in the 1970s.\",\n",
       "  'embedding': tensor([ 0.1460, -0.1540, -0.0707,  ..., -0.0021,  0.0234, -0.0228]),\n",
       "  'text_start': 1,\n",
       "  'text_end': 841},\n",
       " {'content': 'Marvin Minsky and Seymour Papert\\'s 1969 book \"Perceptrons\" highlighted fundamental limitations of single-layer networks, particularly their inability to solve the XOR problem. This led to reduced funding and interest in neural network research, a period known as the \"AI Winter.\"\\n\\nRenaissance (1980s-1990s):\\nThe field experienced a revival with several breakthrough developments:\\n1. The backpropagation algorithm became widely recognized as',\n",
       "  'embedding': tensor([ 0.1483, -0.1504, -0.0732,  ...,  0.0023,  0.0161, -0.0201]),\n",
       "  'text_start': 842,\n",
       "  'text_end': 1282},\n",
       " {'content': 'a solution for training multi-layer networks\\n2. Improvements in computer processing power made larger networks feasible\\n3. New architectures like Convolutional Neural Networks (CNNs) emerged\\n4. Successful applications in pattern recognition and speech processing demonstrated practical value\\n\\nModern Era (2000s-Present):\\nThe explosion of big data and computational power has led to remarkable achievements:\\n- Deep learning models have surpassed human performance in various tasks\\n- Applications range from computer vision to natural language processing\\n- Transfer learning has enabled more efficient model training\\n- Architectures like transformers have revolutionized language models\\n\\nTechnical Foundations:\\n\\nNeural networks consist of',\n",
       "  'embedding': tensor([ 0.1434, -0.1384, -0.0763,  ...,  0.0017,  0.0181, -0.0184]),\n",
       "  'text_start': 1283,\n",
       "  'text_end': 2019},\n",
       " {'content': 'interconnected layers of nodes, each performing weighted calculations:\\n1. Input Layer: Receives raw data\\n2. Hidden Layers: Process information through weighted connections\\n3. Output Layer: Produces final results\\n\\nKey concepts include:\\n- Activation functions (ReLU, sigmoid, tanh)\\n- Weight initialization and adjustment\\n- Loss functions and optimization algorithms\\n- Regularization techniques\\n\\nPractical Applications:\\n\\nModern neural networks have found applications across numerous fields',\n",
       "  'embedding': tensor([ 0.1291, -0.1221, -0.0725,  ...,  0.0022,  0.0129, -0.0149]),\n",
       "  'text_start': 2020,\n",
       "  'text_end': 2507},\n",
       " {'content': ':\\n* Healthcare: Disease diagnosis, drug discovery, medical image analysis\\n* Finance: Risk assessment, fraud detection, algorithmic trading\\n* Transportation: Autonomous vehicles, traffic prediction, route optimization\\n* Entertainment: Content recommendations, game AI, art generation\\n\\nChallenges and Future Directions:\\n\\nDespite their success, neural networks face several ongoing challenges:\\n1. Interpretability and explainability of decisions\\n2. Energy consumption and computational requirements\\n3. Data privacy and ethical considerations\\n4. Robustness against adversarial attacks\\n\\nResearch continues in areas such as:\\n- More efficient architectures\\n- Unsupervised learning approaches\\n- Neuromorphic computing\\n- Integration with symbolic AI systems\\n\\nThe field of neural networks continues to evolve rapidly, with new architectures and applications emerging regularly. As our understanding of both biological and artificial neural networks deepens, we can expect further innovations in this transformative technology.',\n",
       "  'embedding': tensor([ 0.1255, -0.1423, -0.0708,  ...,  0.0022,  0.0204, -0.0192]),\n",
       "  'text_start': 2507,\n",
       "  'text_end': 3523}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "for start_idx, end_idx in boundaries:\n",
    "    text_start = offsets_mapping[start_idx][0].item()\n",
    "    text_end = offsets_mapping[end_idx][1].item()\n",
    "    chunk_text = docs[0][text_start:text_end]\n",
    "    \n",
    "    model_output = token_embeddings[start_idx:end_idx + 1].unsqueeze(0)\n",
    "    chunk_attention_mask = attention_mask[start_idx:end_idx + 1].unsqueeze(0)\n",
    "\n",
    "    chunk_embedding = mean_pooling(model_output, chunk_attention_mask)\n",
    "    chunk_embedding = F.normalize(chunk_embedding, p=2, dim=1)[0]\n",
    "    \n",
    "    chunk = {\n",
    "        \"content\": chunk_text,\n",
    "        \"embedding\": chunk_embedding,\n",
    "        \"text_start\": text_start, \n",
    "        \"text_end\": text_end\n",
    "    }\n",
    "    chunks.append(chunk)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1024]), torch.Size([740, 64]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=64, random_state=411)\n",
    "kmeans.fit(token_embeddings.cpu().numpy())\n",
    "\n",
    "# Normalize the token embeddings and cluster centers\n",
    "normalized_token_embeddings = F.normalize(token_embeddings, p=2, dim=1)\n",
    "concepts = F.normalize(torch.tensor(kmeans.cluster_centers_), p=2, dim=1)\n",
    "\n",
    "# Compute the cosine similarity\n",
    "concepts_target = torch.mm(normalized_token_embeddings, concepts.T)\n",
    "\n",
    "concepts.shape, concepts_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries  = [\n",
    "    \"What were the major developments in neural networks during the 1980s and 1990s?\",\n",
    "    \"Explain the basic components of a neural network's architecture.\",\n",
    "    \"What are the current applications of neural networks in healthcare?\",\n",
    "    \"What caused the AI Winter in the 1970s?\",\n",
    "    \"What are the main challenges facing neural networks today?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_token_embeddings, _, query_attention_mask = get_embedding(queries[1:2])\n",
    "query_embedding = mean_pooling(query_token_embeddings.unsqueeze(0), query_attention_mask.unsqueeze(0))\n",
    "query_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk: 0\n",
      "Similarity: 0.27758681774139404\n",
      "\n",
      "chunk: 1\n",
      "Similarity: 0.27326375246047974\n",
      "\n",
      "chunk: 2\n",
      "Similarity: 0.2968369722366333\n",
      "\n",
      "chunk: 3\n",
      "Similarity: 0.2944921851158142\n",
      "\n",
      "chunk: 4\n",
      "Similarity: 0.2731066942214966\n",
      "\n",
      "most similar chunk:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': 'a solution for training multi-layer networks\\n2. Improvements in computer processing power made larger networks feasible\\n3. New architectures like Convolutional Neural Networks (CNNs) emerged\\n4. Successful applications in pattern recognition and speech processing demonstrated practical value\\n\\nModern Era (2000s-Present):\\nThe explosion of big data and computational power has led to remarkable achievements:\\n- Deep learning models have surpassed human performance in various tasks\\n- Applications range from computer vision to natural language processing\\n- Transfer learning has enabled more efficient model training\\n- Architectures like transformers have revolutionized language models\\n\\nTechnical Foundations:\\n\\nNeural networks consist of',\n",
       " 'embedding': tensor([ 0.1434, -0.1384, -0.0763,  ...,  0.0017,  0.0181, -0.0184]),\n",
       " 'text_start': 1283,\n",
       " 'text_end': 2019}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity between query embedding and chunk embeddings\n",
    "similarities = cosine_similarity(query_embedding.cpu().numpy(), np.stack([chunk['embedding'].cpu().numpy() for chunk in chunks]))\n",
    "\n",
    "for i, similarity in enumerate(similarities[0]):\n",
    "    print(f\"chunk: {i}\")\n",
    "    print(f\"Similarity: {np.max(similarity)}\")\n",
    "    print()\n",
    "    \n",
    "# Get the most similar chunks\n",
    "print(\"most similar chunk:\")\n",
    "most_similar_idx = np.argmax(similarities)\n",
    "most_similar_chunk = chunks[most_similar_idx]\n",
    "\n",
    "most_similar_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fleut\\AppData\\Local\\Temp\\ipykernel_21380\\3684567433.py:6: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  concepts_target_np = np.array(concepts_target)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "concept_similarities = cosine_similarity(query_embedding.cpu().numpy(), concepts.cpu().numpy())\n",
    "\n",
    "concepts_target_np = np.array(concepts_target)\n",
    "concept_similarities_np = np.array(concept_similarities.T)\n",
    "token_importances = np.dot(concepts_target_np, concept_similarities_np)[:,0]\n",
    "token_importances = (token_importances - np.min(token_importances)) / (np.max(token_importances) - np.min(token_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dense_subsequences(\n",
    "    values: np.ndarray,\n",
    "    min_size: int = 1,\n",
    "    max_size: int = None,\n",
    "    num_sequences: int = 3,\n",
    "    min_density: float = None,\n",
    "    min_gap: int = 0\n",
    ") -> list[tuple[int, int, float]]:\n",
    "\n",
    "    if max_size is None:\n",
    "        max_size = len(values)\n",
    "        \n",
    "    n = len(values)\n",
    "    results = []\n",
    "    used_positions = np.zeros(n, dtype=bool)\n",
    "\n",
    "    def is_valid_region(start: int, end: int) -> bool:\n",
    "        for s in range(max(0, start - min_gap), min(n, end + min_gap)):\n",
    "            if used_positions[s]:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    cumsum = np.concatenate(([0], np.cumsum(values)))\n",
    "    \n",
    "    while len(results) < num_sequences:\n",
    "        max_density = float('-inf')\n",
    "        best_start = None\n",
    "        best_end = None\n",
    "        \n",
    "\n",
    "        for length in range(min_size, min(n + 1, max_size + 1)):\n",
    "            for start in range(n - length + 1):\n",
    "                end = start + length\n",
    "\n",
    "                if not is_valid_region(start, end):\n",
    "                    continue\n",
    "\n",
    "                curr_sum = cumsum[end] - cumsum[start]\n",
    "                density = curr_sum / length\n",
    "                \n",
    "                if density > max_density:\n",
    "                    max_density = density\n",
    "                    best_start = start\n",
    "                    best_end = end\n",
    "\n",
    "        if best_start is None or (min_density is not None and max_density < min_density):\n",
    "            break\n",
    "\n",
    "        used_positions[best_start:best_end] = True\n",
    "        results.append((best_start, best_end, max_density))\n",
    "\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 dense passages:\n",
      "Dense Passage: er networks\n",
      "2. Improvements in computer processing power made larger networks feasible\n",
      "3. New a\n",
      "Density: 0.8834032331194196\n",
      "\n",
      "Dense Passage: performance in various tasks\n",
      "- Applications range from computer vision to natural language processing\n",
      "- Transfer learning has enabled more efficient model\n",
      "Density: 0.8782475789388021\n",
      "\n",
      "Dense Passage: consumption and computational requirements\n",
      "3. Data privacy and ethical considerations\n",
      "4. Robustness against adversarial attack\n",
      "Density: 0.8738695227581522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concepts_results = find_dense_subsequences(token_importances, min_size=20, max_size=100, num_sequences=3, min_density=0.2, min_gap=10)\n",
    "print(f\"Found {len(concepts_results)} dense passages:\")\n",
    "for start, end, density in concepts_results:\n",
    "    text_start = offsets_mapping[start][0].item()\n",
    "    text_end = offsets_mapping[end][1].item()\n",
    "    dense_passage = docs[0][text_start:text_end]\n",
    "    print(f\"Dense Passage: {dense_passage}\")\n",
    "    print(f\"Density: {density}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
